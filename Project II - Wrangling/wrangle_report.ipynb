{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRateDogs Twitter Archive Wrangling Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "WeRateDogs is a social media page that rates people’s dogs with humorous comments. It was founded by Matt Nelson in 2015 and has since become an authority in its niche. Aside from rating people’s dogs, the page also helps animal rights organisations such as ASPCA (American Society for the Prevention of Cruelty to Animals) raise funds and run GoFundMe campaigns for dogs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "The main data required for this project was WeRateDogs’ Twitter archive, which WeRateDog itself provided to Udacity.\n",
    "Two additional datasets were generated based on the Twitter archive.\n",
    "\n",
    "**Image prediction**: This data was generated by running every image in the WeRateDogs Twitter archive through a neural network that can classify breeds of dogs.\n",
    "\n",
    "The variables in this data are:\n",
    "\n",
    "- tweet_id is the last part of the tweet URL after \"status/\"\n",
    "- jpg_url is the image URL\n",
    "- img_num is the image number corresponding to the most confident prediction (numbered 1 to 4 since tweets can have up to four images).\n",
    "- p1 is the algorithm's #1 prediction for the image in the tweet\n",
    "- p1_conf is how confident the algorithm is in its #1 prediction\n",
    "- p1_dog is whether or not the #1 prediction is a breed of dog\n",
    "- p2 is the algorithm's second most likely prediction\n",
    "- p2_conf is how confident the algorithm is in its #2 prediction \n",
    "- p2_dog is whether or not the #2 prediction is a breed of dog \n",
    "- p3 is the algorithm's third most likely prediction\n",
    "- p3_conf is how confident the algorithm is in its #3 prediction \n",
    "- p3_dog is whether or not the #3 prediction is a breed of dog\n",
    "\n",
    "**Tweet_df**: This data was generated from querying Twitter API for WeRateDogs’ Twitter archive \n",
    "\n",
    "The variables in this data are:\n",
    "\n",
    "- tweet_id\n",
    "- retweet_count is the number of times a tweet was retweeted\n",
    "- favourite_count is the number of times a tweet was favourited. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Data\n",
    "The author employed both visual and programmatic assessments to review the three pieces of data for quality and tidiness issues. A methodical approach was taken to visually and programmatically assess each of the data on Microsoft Excel; reviewing each table a column at a time.  \n",
    "\n",
    "With visual assessment, the following issues were discovered:\n",
    "\n",
    "**Data quality**;\n",
    "- source variable appear as ``<a>`` tag instead of device type;\n",
    "- expanded_urls contain multiple urls in one row;\n",
    "- in_reply_to_status_id and in_reply_to_user_id contain rows that are replies and not original tweets;\n",
    "- retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp contains rows that are retweets and not original tweets \n",
    "- The breed of some dog images were not predicted. That is p1_dog, p2_dog, and p3_dog are all false\n",
    "\n",
    "**Tidiness issues**;\n",
    "- Some tweets had more than one dog stage; and\n",
    "- One variable (dog stage) in three columns (doggo, floofer, pupper, puppo).\n",
    "- Two variables in nine columns (dog breed and prediction confidence)\n",
    "- ``image_prediction`` should be part of ``twitter_archive`` table\n",
    "- ``tweet_df`` should be part of ``twitter_archive`` table\n",
    "\n",
    "The author used `describe()`, `sample()`, and `info()` functions to programmatically assess the three data. Here are what were discovered:\n",
    "\n",
    "**Data quality**;\n",
    "- Erroneous datatypes for tweet_id, rating_numerator, and timestamp;\n",
    "- a, an, very, just, by, my, o, this as dog names;\n",
    "- Digits after the decimal point were omitted for rating_numerator; and\n",
    "- Missing records in ``image_prediction``: 2075 instead of 2356.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "Before the author began cleaning the data, the author made copies of the original datasets, using ``copy()`` function:\n",
    "```python\n",
    "twitter_archive_clean = twitter_archive.copy()\n",
    "image_prediction_clean = image_prediction.copy()\n",
    "tweet_df_clean = tweet_df.copy()\n",
    "```\n",
    "\n",
    "Cleaning was performed one table at a time, starting from ``twitter_archive ``to ``tweet_df``"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``twitter_archive``: data quality and tidiness resolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, the tidiness issue of one variable (`dog_stage`) in four columns and the data quality issue of tweets having multiple dog stages were resolved. \n",
    "\n",
    "1. A new table (``dog_stage_df``) was created by applying ``pd.melt()`` on columns tweet_id, doggo, floofer, pupper, puppo\n",
    "    ```python\n",
    "    dog_stage_df = pd.melt(twitter_archive_clean, id_vars='tweet_id', value_vars=['doggo', 'floofer', 'pupper', 'puppo'], value_name='dog_stage')\n",
    "    ```\n",
    "\n",
    "2. The variable column and duplicates rows resulting from melting those columns were dropped using ``drop()`` and ``drop_duplicates()``, and None values in the dog_stage column were then replaced with ``np.NAN``\n",
    "    \n",
    "    ```python\n",
    "    dog_stage_df.drop('variable', axis=1, inplace=True)\n",
    "    dog_stage_df.drop_duplicates(inplace=True)\n",
    "    dog_stage_df['dog_stage'] = dog_stage_df['dog_stage'].replace('None', np.NAN)\n",
    "    ```\n",
    "\n",
    "3. The author combined the stages for tweets with more than one dog stage and dropped the duplicate values in the ``dog_stage_df`` table\n",
    "\n",
    "    ```python\n",
    "    with_dog_stages = dog_stage_df[dog_stage_df.dog_stage.notna()]\n",
    "    combined_stages = with_dog_stages.groupby('tweet_id').transform(lambda x: ', '.join(x))\n",
    "    dog_stage_df.loc[combined_stages.index, 'dog_stage'] = combined_stages.values\n",
    "    dog_stage_df.sort_values(by='dog_stage', inplace=True)\n",
    "    dog_stage_df.drop_duplicates(subset='tweet_id', inplace=True)\n",
    "    ```\n",
    "\n",
    "4. ``dog_stage_df`` was merged to the twitter_archive using ``pd.merge()`` with left join\n",
    "\n",
    "    ```python\n",
    "    twitter_archive_clean = pd.merge(twitter_archive_clean, dog_stage_df, how='left', on='tweet_id')\n",
    "    twitter_archive_clean.drop(['doggo', 'floofer', 'pupper', 'puppo'], axis=1, inplace=True)\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second, the author addressed the data quality issue of tweets that were replies and retweets  \n",
    "\n",
    "1. Rows that were replies were removed using ``isnull()`` and columns in_reply_to_status_id and in_reply_to_user_id were dropped\n",
    "    \n",
    "    ```python\n",
    "    twitter_archive_clean = twitterarchive_clean[twitter_archive_clean['in_reply_to_user_id'].isnull()]\n",
    "    twitter_archive_clean.drop(['in_reply_to_status_id', 'in_reply_to_user_id'], axis=1, inplace=True)\n",
    "    ```\n",
    "\n",
    "2. Rows that were retweets were removed using ``isnull()`` and columns retweeted_status_id, retweeted_status_user_id, and retweeted_status_timestamp were dropped\n",
    "\n",
    "    ```python\n",
    "    twitter_archive_clean = twitter_archive_clean[twitter_archive_clean['retweeted_status_user_id'].isnull()]\n",
    "    twitter_archive_clean.drop(['retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp'], axis=1, inplace=True)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third, the author dealt with the data quality issue of digits after the decimal point being omitted for the variable ``rating_numerator``\n",
    "\n",
    "1. The rating numerator for each tweet was extracted from the tweets using extact() and regex\n",
    "\n",
    "    ```python\n",
    "    twitter_archive_clean['rating_numerator'] = twitter_archive_clean['text'].str.extract('(\\d+\\.*\\d*\\/\\d+)', expand=False).str.split('/').str[0]\n",
    "    ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, the last two data quality issues (source is coded as ``<a>`` tag instead of device type and expanded urls contain multiple urls in one row) in the ``twitter_archive`` table were resolved\n",
    "\n",
    "1. The device types were extracted from the source url using ``extract()`` and regex\n",
    "    \n",
    "    ```python\n",
    "    twitter_archive_clean['source'] = twitter_archive_clean['source'].str.extract('^<a.+>(.+)</a>$')\n",
    "    ```\n",
    "\n",
    "2. The first urls in expanded_url's rows were retained and the others removed using a combination of ``split()`` and list slicing\n",
    "\n",
    "    ```python\n",
    "    twitter_archive_clean['expanded_urls'] = twitter_archive_clean['expanded_urls'].str.split(',').str[0]\n",
    "    ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``image_prediction``: data quality and tidiness resolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, the author addressed the tidiness issue: two variables in nine columns (dog breed and prediction confidence)\n",
    "\n",
    "The issue was resolved using ``np.select()``, which takes a condition list and a choice list.\n",
    "\n",
    "1. The author created a condition list for dog breed and a choice list for dog breed and prediction confidence\n",
    "\n",
    "    ```python\n",
    "    condlist = [(image_prediction_clean['p1_dog'] == True), (image_prediction_clean['p2_dog'] == True), (image_prediction_clean['p3_dog'] == True)]\n",
    "    choicelist = [image_prediction_clean['p1'], image_prediction_clean['p2'], image_prediction_clean['p3']]\n",
    "    choicelist_2 = [image_prediction_clean['p1_conf'], image_prediction_clean['p2_conf'], image_prediction_clean['p3_conf']]\n",
    "    ```\n",
    "\n",
    "2. Then ``np.select()`` was used to create dog_breed and picture columns variables\n",
    "\n",
    "    ```python\n",
    "    image_prediction_clean['breed'] = np.select(condlist, choicelist, default='Unknown')\n",
    "    image_prediction_clean['pix_conf'] = np.select(condlist, choicelist_2, default=0)\n",
    "    ```\n",
    "\n",
    "3. The columns in ``image_prediction`` table were dropped\n",
    "\n",
    "    ```python\n",
    "    image_prediction_clean.drop(['img_num', 'p1', 'p1_dog', 'p1_conf', 'p2', 'p2_dog', 'p2_conf', 'p3', 'p3_dog', 'p3_conf'], axis=1, inplace=True)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second, ``image_prediction`` table was merged with the ``twitter_archive table``\n",
    "\n",
    "1. The author used ``pd.merge()`` to left merge ``twitter_archive`` to ``image_prediction`` using tweet_id\n",
    "\n",
    "    ```python\n",
    "    twitter_archive_clean = pd.merge(twitter_archive_clean, image_prediction_clean, how='left', on='tweet_id')\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The author went back to the ``twitter_archive`` table after merging it with ``image_prediction`` table to fix the erroneous datatype issues\n",
    "\n",
    "1. The author set the right datatype for tweet_id, rating_numerator, and dog_stage using ``astype()``\n",
    "\n",
    "    ```python\n",
    "    twitter_archive_clean['tweet_id'] = twitter_archive_clean['tweet_id'].astype(str)\n",
    "    twitter_archive_clean['rating_numerator'] = twitter_archive_clean['rating_numerator'].astype(float)\n",
    "    twitter_archive_clean[['dog_stage', 'breed']] = twitter_archive_clean[['dog_stage', 'breed']].astype('category')\n",
    "    ```\n",
    "\n",
    "2. timestamp was converted to datetype using ``pd.to_datetype()``\n",
    "\n",
    "    ```python\n",
    "    twitter_archive_clean['timestamp'] = pd.to_datetime(twitter_archive_clean['timestamp'])\n",
    "    ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``tweet_df``: data quality and tidiness resolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The only issue the author had to fix when it comes to tweet_df is left merging it to twitter_archive\n",
    "\n",
    "    ```python\n",
    "    twitter_archive_clean = pd.merge(twitter_archive_clean, tweet_df_clean, how='left', on='tweet_id')\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that done, all data quality and tidiness issues with the dataset were resolved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The author saved the master dataset to a sqlite3 database\n",
    "\n",
    "1. Imported sqlite3, defined table name, and set the column names\n",
    "\n",
    "    ```python\n",
    "    import sqlite3\n",
    "    table_name = 'master'\n",
    "    col_names = ['tweet_id', 'timestamp', 'source', 'text', 'expanded_urls',\n",
    "        'rating_numerator', 'rating_denominator', 'name', 'dog_stage',\n",
    "        'jpg_url', 'breed', 'pix_conf', 'retweet_count', 'favorite_count']\n",
    "    ```\n",
    "\n",
    "2. The sqlite3 connection and query were defined and to_sql() was used to store the dataset in a sqlite3 database\n",
    "\n",
    "    ```python\n",
    "    conn = sqlite3.connect('twitter_archive_master.sqlite')\n",
    "    query = f'Create table if not Exists {table_name} (col_names)'\n",
    "    conn.execute(query)\n",
    "    twitter_archive_clean.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    conn.commit()\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cf1ee66cb0a5dbbe9b15b35d8b5ef163dfe6de55254de5ac3d70cf481ad0a057"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
